{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a Transformer-based architecture on the IMDB Movie Reviews dataset for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies, save requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.16.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas tqdm\n",
    "!pip install -U torch \n",
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "tqdm\n",
    "torch==1.1.0\n",
    "pytorch_transformers\n",
    "pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# path to data \n",
    "DATA_DIR = os.path.abspath('./data')\n",
    "\n",
    "# path to IMDB\n",
    "IMDB = os.path.join(DATA_DIR, \"aclImdb\")\n",
    "\n",
    "# url to dataset\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "def download_url(url:str, dest:str, overwrite:bool=True, show_progress=True, \n",
    "                 chunk_size=1024*1024, timeout=4, retries=5)->None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    \n",
    "    dest = os.path.join(dest, os.path.basename(url))\n",
    "    if os.path.exists(dest) and not overwrite: \n",
    "        print(f\"File {dest} already exists!\")\n",
    "        return dest\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    print(f\"Downloading {url}\")\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = tqdm(range(file_size), leave=False)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Download failed after {retries} retries.\")\n",
    "            import sys;sys.exit(1)\n",
    "        finally:\n",
    "            return dest\n",
    "        \n",
    "def untar(file_path, dest:str):\n",
    "    \"Untar `file_path` to `dest`\"\n",
    "    print(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eab3f53b5846bb98a637d37aa63f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84125825), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untar aclImdb_v1.tar.gz to /Users/d069049/Develop/transformer-finetuning/data\n"
     ]
    }
   ],
   "source": [
    "# download imdb dataset\n",
    "file_path = download_url(url, '/tmp', overwrite=True)\n",
    "\n",
    "# untar imdb dataset to DATA_DIR\n",
    "untar(file_path, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\r\n",
      "-rw-r--r--. 1 7297 1000 882K Jun 11  2011 imdbEr.txt\r\n",
      "-rw-r--r--. 1 7297 1000 827K Apr 12  2011 imdb.vocab\r\n",
      "-rw-r--r--. 1 7297 1000 4.0K Jun 26  2011 README\r\n",
      "drwxr-xr-x. 4 7297 1000  115 Apr 12  2011 test\r\n",
      "drwxr-xr-x. 5 7297 1000  183 Jun 26  2011 train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "\n",
    "def read_imdb(imdb_dir: str, max_lengths={\"train\": None, \"test\": None}):\n",
    "    \"Read imdb data to a DataFrame in {'label', 'text'} format.\"\n",
    "    imdb_dir = Path(imdb_dir)\n",
    "    datasets = {}\n",
    "    \n",
    "    for t in [\"train\", \"test\"]:\n",
    "        texts, labels = [], []\n",
    "        for p in [\"pos\", \"neg\"]:\n",
    "            for file in tqdm((imdb_dir/\"train\"/p).glob(\"*.txt\"), desc=f\"reading {t}/{p}\"):\n",
    "                with open(file, 'r') as fin:\n",
    "                    text = fin.readlines()[0].replace(r'\\n', ' ')\n",
    "                    text = clean_html(text).strip()\n",
    "                    texts +=  [text]\n",
    "                    labels += [p]\n",
    "        df = pd.DataFrame(\n",
    "            {LABEL_COL: labels, TEXT_COL: texts})\n",
    "        \n",
    "        max_length = max_lengths.get(t)\n",
    "        if max_length is not None and max_length <= len(df):\n",
    "            # pick max_length\n",
    "            datasets[t] = df.sample(n=max_length)\n",
    "        else:\n",
    "            # just shuffle\n",
    "            datasets[t] = df.sample(frac=1)\n",
    "            \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397379eed6754aa5997eaf9045857a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading train/pos', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961902b2dd09489f956727c9dee41938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading train/neg', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ea80b2a70e4507a39f4595be722e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading test/pos', max=1, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230efb073a3741f096dd66e190e5d9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading test/neg', max=1, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_TRAIN = 5000\n",
    "MAX_TEST = 5000\n",
    "\n",
    "# read data, 5000-5000 each\n",
    "datasets = read_imdb(IMDB, max_lengths={\"train\": MAX_TRAIN, \"test\": MAX_TEST})\n",
    "\n",
    "# list of labels\n",
    "labels = list(set(datasets[\"train\"][LABEL_COL].tolist()))\n",
    "\n",
    "# labels to integers mapping\n",
    "label2int = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = datasets[\"train\"]\n",
    "df_test = datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"imdb5k_train.csv\", index=False)\n",
    "df_test.to_csv(\"imdb5k_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv(\"imdb5k_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12468</th>\n",
       "      <td>pos</td>\n",
       "      <td>There's a great deal of material from the Mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11118</th>\n",
       "      <td>pos</td>\n",
       "      <td>Cliffhanger is a decent action crime adventure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>pos</td>\n",
       "      <td>In terms of the arts, the 1970s were a very tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>pos</td>\n",
       "      <td>Overall, I agree wholly with Ebert's review. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19117</th>\n",
       "      <td>neg</td>\n",
       "      <td>I watched this movie after having so much of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "12468   pos  There's a great deal of material from the Mode...\n",
       "11118   pos  Cliffhanger is a decent action crime adventure...\n",
       "3410    pos  In terms of the arts, the 1970s were a very tu...\n",
       "5786    pos  Overall, I agree wholly with Ebert's review. I...\n",
       "19117   neg  I watched this movie after having so much of t..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "NUM_MAX_POSITIONS = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class TextProcessor:\n",
    "    \n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    \n",
    "    def __init__(self, tokenizer, label2id: dict, num_max_positions:int=512):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "        \n",
    "    \n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions-1] \n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        \n",
    "        return ids, self.label2id[label]\n",
    "    \n",
    "# download the 'bert-base-cased' tokenizer\n",
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "# initialize a TextProcessor\n",
    "processor = TextProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningConfig(num_classes=2, dropout=0.1, init_range=0.02, batch_size=32, lr=6.5e-05, max_norm=1.0, n_epochs=2, n_warmup=10, valid_pct=0.1, gradient_acc_steps=1, device=device(type='cuda', index=0), log_dir='./logs/', dataset_cache='./cache/dataset_cache.bin')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "LOG_DIR = \"./logs/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FineTuningConfig = namedtuple('FineTuningConfig',\n",
    "      field_names=\"num_classes, dropout, init_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "                  \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir, dataset_cache\")\n",
    "\n",
    "finetuning_config = FineTuningConfig(\n",
    "                2, 0.1, 0.02, BATCH_SIZE, 6.5e-5, 1.0, 2,\n",
    "                10, 0.1, 1, device, LOG_DIR, \n",
    "                CACHE_DIR+'dataset_cache.bin')\n",
    "\n",
    "finetuning_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df: pd.DataFrame, processor: TextProcessor, batch_size:int=32, shuffle:bool=False, valid_pct:float=None, \n",
    "                   text_col:str=\"text\", label_col:str=\"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a  DataLoader\"\n",
    "    \n",
    "    features, labels = [], [] \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        ids, lbl = processor.process_example((row[LABEL_COL], row[TEXT_COL]))\n",
    "        features += [ids]\n",
    "        labels += [lbl]\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "                    torch.tensor(features, dtype=torch.long), \n",
    "                    torch.tensor(labels, dtype=torch.long))\n",
    "    \n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset, [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    \n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238b9a6e6f9e43228505aab39016572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc060bf9ff49eab7047ad6fdfe51a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create train and valid sets by splitting\n",
    "train_dl, valid_dl = create_dataloaders(datasets[\"train\"], processor, \n",
    "                                    batch_size=finetuning_config.batch_size, \n",
    "                                    valid_pct=finetuning_config.valid_pct)\n",
    "\n",
    "test_dl = create_dataloaders(datasets[\"test\"], processor, \n",
    "                             batch_size=finetuning_config.batch_size, \n",
    "                             valid_pct=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerWithClfHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     6,
     52
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_num_params(model):\n",
    "    mp = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum(np.prod(p.size()) for p in mp)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       fine_tuning_config.dropout, causal=not config.mlm)\n",
    "        \n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters discarded from the pretrained model: ['lm_head.weight']\n",
      "Parameters added in the model: ['classification_head.weight', 'classification_head.bias']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import cached_path\n",
    "\n",
    "# download pre-trained model and config\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location='cpu')\n",
    "\n",
    "config = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n",
    "\n",
    "# init model: Transformer base + classifier head\n",
    "model = TransformerWithClfHead(config=config, fine_tuning_config=finetuning_config).to(finetuning_config.device)\n",
    "\n",
    "incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Parameters discarded from the pretrained model: {incompatible_keys.unexpected_keys}\")\n",
    "print(f\"Parameters added in the model: {incompatible_keys.missing_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50397182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     38
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy \n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "\n",
    "# Bert optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=finetuning_config.lr, correct_bias=False) \n",
    "\n",
    "def update(engine, batch):\n",
    "    \"update function for training\"\n",
    "    model.train()\n",
    "    inputs, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "    inputs = inputs.transpose(0, 1).contiguous() # [S, B]\n",
    "    _, loss = model(inputs, \n",
    "                    clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]), \n",
    "                    clf_labels=labels)\n",
    "    loss = loss / finetuning_config.gradient_acc_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), finetuning_config.max_norm)\n",
    "    if engine.state.iteration % finetuning_config.gradient_acc_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    \"update function for evaluation\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()\n",
    "        logits = model(inputs,\n",
    "                       clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]),\n",
    "                       padding_mask = (batch == tokenizer.vocab[processor.PAD]))\n",
    "    return logits, labels\n",
    "\n",
    "def predict(model, tokenizer, int2label, input=\"test\"):\n",
    "    \"predict `input` with `model`\"\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous() # [S, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask = (tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                   padding_mask = (tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()    \n",
    "    return {int2label[val.argmax()]: val.max(),\n",
    "            int2label[val.argmin()]: val.min()}\n",
    "trainer = Engine(update)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# add metric to evaluator \n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "\n",
    "# add evaluator to trainer: eval on valid set after each epoch\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_dl)\n",
    "    print(f\"validation epoch: {engine.state.epoch} acc: {100*evaluator.state.metrics['accuracy']}\")\n",
    "          \n",
    "# lr schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (finetuning_config.n_warmup, finetuning_config.lr),\n",
    "                                              (len(train_dl)*finetuning_config.n_epochs, 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "\n",
    "# add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(finetuning_config.log_dir, 'finetuning_checkpoint', \n",
    "                                     save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'imdb_model': model})\n",
    "\n",
    "# save config to logdir\n",
    "torch.save(finetuning_config, os.path.join(finetuning_config.log_dir, 'fine_tuning_args.bin'))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets fine-tune on imdb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d342879fdf8a4ffd96a8a44818a68b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 acc: 84.2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea04dae93c9d4c148600b9727ceec029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 acc: 86.4\n",
      "\n",
      "test results - acc: 89.800\n"
     ]
    }
   ],
   "source": [
    "# fit the model on train_dl\n",
    "trainer.run(train_dl, max_epochs=finetuning_config.n_epochs)\n",
    "\n",
    "# evaluate the model on test_dl\n",
    "evaluator.run(test_dl)\n",
    "print(f\"test results - acc: {100*evaluator.state.metrics['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 196912\r\n",
      "-rw-r--r--. 1 root root       318 Jul 17 09:56 fine_tuning_args.bin\r\n",
      "-rw-------. 1 root root 201630224 Jul 17 09:59 finetuning_checkpoint_imdb_model_2.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $finetuning_config.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int2label = {i:label for label,i in label2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.9117301, 'neg': 0.08826993}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"I just love how the actors are playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.9916163, 'pos': 0.008383713}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"This movie is poorly directed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bottlepy.org/bottle.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
