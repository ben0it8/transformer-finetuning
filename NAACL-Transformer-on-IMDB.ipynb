{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* use https://colab.research.google.com/drive/1iDHCYIrWswIKp-n-pOg69xLoZO09MEgf#scrollTo=zjzTkJGl1J0l this model and arch instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.16.3)\n",
      "Requirement already satisfied: pytorch_pretrained_bert in /opt/conda/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2019.6.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2.21.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (4.32.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.9.160)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.16.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.160 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (1.12.160)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.160->boto3->pytorch_pretrained_bert) (2.8.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.160->boto3->pytorch_pretrained_bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.160->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "Requirement already satisfied: pytorch-ignite in /opt/conda/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pytorch-ignite) (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->pytorch-ignite) (1.16.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install pytorch_pretrained_bert\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import tarfile\n",
    "import urllib\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data').resolve()\n",
    "IMDB = DATA_DIR/'aclImdb'\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def download_url(url:str, dest:str, overwrite:bool=True, show_progress=True, \n",
    "                 chunk_size=1024*1024, timeout=4, retries=5)->None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    dest = Path(dest)/os.path.basename(url)\n",
    "    if os.path.exists(dest) and not overwrite: \n",
    "        print(\"File already existing\")\n",
    "        return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    print(f\"Downloading {url}\")\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = tqdm(range(file_size), leave=False)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Download failed after {retries} retries.\")\n",
    "            import sys;sys.exit(1)\n",
    "        finally:\n",
    "            return str(dest)\n",
    "        \n",
    "def untar(file_path, dest:str):\n",
    "    print(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))\n",
    "    os.remove(file_path)\n",
    "    return str(dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152201ecf9104e6d8695967daec40a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84125825), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untar aclImdb_v1.tar.gz to /workspace/data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/data'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = download_url(url, '/tmp', overwrite=True)\n",
    "untar(file_path, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 63M\r\n",
      "drwxr-xr-x. 4 root root 105 Jun 26  2011 aclImdb\r\n",
      "-rw-r--r--. 1 root root 32M Jul  4 14:43 dev.tsv\r\n",
      "-rw-r--r--. 1 root root 32M Jul  4 14:43 train.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_html(raw):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "\n",
    "def read_imdb(imdb_dir: str, text_col='text', label_col='label'):\n",
    "    \"Read imdb data to {'label', 'text'} format\"\n",
    "    imdb_dir = Path(imdb_dir)\n",
    "    datasets = {}\n",
    "    \n",
    "    for t in ['train', 'test']:\n",
    "        texts, labels = [], []\n",
    "        for p in ['pos', 'neg']:\n",
    "            for file in tqdm((imdb_dir/'train'/p).glob(\"*.txt\"), desc=f'reading {t}/{p}'):\n",
    "                with open(file, 'r') as fin:\n",
    "                    text = fin.readlines()[0].replace(r'\\n', ' ')\n",
    "                    text = clean_html(text).strip()\n",
    "                    texts +=  [text]\n",
    "                    labels += [p]\n",
    "        df = pd.DataFrame(\n",
    "            {label_col: labels, text_col: texts})\n",
    "        datasets[t] = df.sample(frac=1)\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading train/pos: 12500it [00:01, 9103.39it/s]\n",
      "reading train/neg: 12500it [00:01, 9231.60it/s]\n",
      "reading test/pos: 12500it [00:01, 9245.09it/s]\n",
      "reading test/neg: 12500it [00:01, 9255.92it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = read_imdb(IMDB)\n",
    "\n",
    "labels = list(set(datasets['train']['label'].tolist()))\n",
    "label2int = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs - args: base model parameters, adapt_args: fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "LOG_DIR = \"./logs/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "\n",
    "NUM_MAX_POSITIONS = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "AdaptationConfig = namedtuple('AdaptationConfig',\n",
    "  field_names=\"num_classes, dropout, initializer_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "              \"n_warmup, valid_pct, gradient_accumulation_steps, device,\"\n",
    "              \"log_dir, dataset_cache\")\n",
    "adapt_args = AdaptationConfig(\n",
    "               2          , 0.1    , 0.02             , 32        , 6.5e-5, 1.0   , 3,\n",
    "               10      , 0.1           , 1, device,\n",
    "               LOG_DIR, CACHE_DIR+'dataset_cache.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     7,
     43
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    \n",
    "    def __init__(self, tokenizer, label2id, num_max_positions=512):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "        \n",
    "    \n",
    "    def process_example(self, example):\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        \n",
    "#         with warnings.catch_warnings():\n",
    "#             warnings.simplefilter(\"ignore\")\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions-1] \n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        \n",
    "#         ids =  self.tokenizer.convert_tokens_to_ids(tokens)         \n",
    "#         if len(ids) >= self.num_max_positions:\n",
    "#             ids = ids[:self.num_max_positions-1] + [self.tokenizer.vocab[self.CLS]]\n",
    "#         else:\n",
    "#             pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(ids)-1)\n",
    "#             ids = ids + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        return ids, self.label2id[label]\n",
    "    \n",
    "\n",
    "def create_dataset(df, processor, batch_size=32, shuffle=False, valid_pct=None, \n",
    "                   text_col=\"text\", label_col=\"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a DataLoader\"\n",
    "    \n",
    "    features, labels = [], []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        ids, lbl = processor.process_example([row.label, row.text])\n",
    "        features += [ids]\n",
    "        labels += [lbl]\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "                    torch.tensor(features, dtype=torch.long), \n",
    "                    torch.tensor(labels, dtype=torch.long))\n",
    "    \n",
    "    if valid_pct is not None:\n",
    "        \n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset, [valid_size, train_size])\n",
    "        \n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    \n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "processor = DataProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = datasets[\"train\"].sample(5000)\n",
    "df_test = datasets[\"test\"].sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc87e4e609724ac391bcebc858e027c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbfa817ece24eb7854b454db5d669d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dl, valid_dl = create_dataset(df_train, processor, batch_size=adapt_args.batch_size, \n",
    "                                    valid_pct=adapt_args.valid_pct)\n",
    "\n",
    "test_dl = create_dataset(df_test, processor, batch_size=adapt_args.batch_size, valid_pct=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerWithClfHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_num_params(model):\n",
    "    import numpy as np\n",
    "    mp = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum(np.prod(p.size()) for p in mp)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       fine_tuning_config.dropout, causal=not config.mlm)\n",
    "        \n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters discarded from the pretrained model: ['lm_head.weight']\n",
      "Parameters added in the adaptation model: ['classification_head.weight', 'classification_head.bias']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import cached_path\n",
    "\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location='cpu')\n",
    "\n",
    "args = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n",
    "\n",
    "adaptation_model = TransformerWithClfHead(config=args, fine_tuning_config=adapt_args).to(adapt_args.device)\n",
    "\n",
    "incompatible_keys = adaptation_model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Parameters discarded from the pretrained model: {incompatible_keys.unexpected_keys}\")\n",
    "print(f\"Parameters added in the adaptation model: {incompatible_keys.missing_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50397182"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(adaptation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=410, out_features=2, bias=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptation_model.classification_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [
     7,
     25
    ]
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy \n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "\n",
    "optimizer = torch.optim.Adam(adaptation_model.parameters(), lr=adapt_args.lr)\n",
    "\n",
    "def update(engine, batch):\n",
    "    \n",
    "    adaptation_model.train()\n",
    "    inputs, labels = (t.to(adapt_args.device) for t in batch)\n",
    "    \n",
    "    inputs = inputs.transpose(0, 1).contiguous() # [S, B]\n",
    "    _, loss = adaptation_model(inputs, \n",
    "                               clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]), \n",
    "                               clf_labels=labels)\n",
    "    loss = loss / adapt_args.gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(adaptation_model.parameters(), adapt_args.max_norm)\n",
    "    if engine.state.iteration % adapt_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    adaptation_model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(adapt_args.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()\n",
    "        logits = adaptation_model(inputs,\n",
    "                                  clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]),\n",
    "                                  padding_mask = (batch == tokenizer.vocab[processor.PAD]))\n",
    "    return logits, labels\n",
    "                              \n",
    "trainer = Engine(update)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# Attache metric to evaluator & evaluation to trainer: evaluate on valid set after each epoch\n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_dl)\n",
    "    print(f\"validation epoch: {engine.state.epoch} acc: {100*evaluator.state.metrics['accuracy']}\")\n",
    "          \n",
    "# Learning rate schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (adapt_args.n_warmup, adapt_args.lr),\n",
    "                                              (len(train_dl)*adapt_args.n_epochs, 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "\n",
    "# Add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# Save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(adapt_args.log_dir, 'finetuning_checkpoint', \n",
    "                                     save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': adaptation_model})\n",
    "torch.save(args, os.path.join(adapt_args.log_dir, 'fine_tuning_args.bin'))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets fine-tune on imdb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a4a92c29da41eb8b39ea6a9acd8b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 acc: 84.39999999999999\n",
      "CPU times: user 23.2 s, sys: 41.7 s, total: 1min 4s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f7f004e0fd0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results - acc: 86.860\n"
     ]
    }
   ],
   "source": [
    "evaluator.run(test_dl)\n",
    "print(f\"test results - acc: {100*evaluator.state.metrics['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, int2label, input=\"test\"):\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous() # [S, 1]\n",
    "    logits = adaptation_model(tensor_in,\n",
    "                              clf_tokens_mask = (tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                              padding_mask = (tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()    \n",
    "    return {int2label[val.argmax()]: val.max(),\n",
    "            int2label[val.argmin()]: val.min()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.5452621, 'pos': 0.45473787}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2label = {i:label for label,i in label2int.items()}\n",
    "\n",
    "predict(adaptation_model, tokenizer, int2label, input = \"This movie sucks...SO SHIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(adaptation_model, tokenizer, int2label, input = \"This movie sucks...SO SHIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = F.softmax(val, dim=0).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5452621 0\n",
      "0.45473787 0\n"
     ]
    }
   ],
   "source": [
    "ind = ind.detach().cpu().numpy()\n",
    "\n",
    "for v,i in zip(val, ind):\n",
    "    print(v,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19339132, 0.01184591], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gpu-remote Py 3.6",
   "language": "",
   "name": "rik_ssh_gpu_remote_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
